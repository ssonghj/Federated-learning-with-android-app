{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 uninstall -y tensorflow-federated-nightly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.7.0\n",
      "Tensorflow Federated version: 0.19.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from operator import itemgetter\n",
    "import collections\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.random.set_seed(99)\n",
    "import tensorflow_federated as tff\n",
    "import numpy as np\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "#tf.compat.v1.enable_v2_behavior() # https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_v2_behavior\n",
    "\n",
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "print(f'Tensorflow Federated version: {tff.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.config.per_process_gpu_memory_fraction = 0.85 #메모리 사용율 제한\n",
    "        #logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "# Setup scripts (or notebook)\n",
    "# Global variables\n",
    "# Setup scripts (or notebook)\n",
    "IMG_DATA = './equal_train'\n",
    "IMG_SHAPE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "CLASSES = ['acne', 'melanoma', 'psoriasis', 'wart']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: /home/dnlab/Desktop/20211024T133959/equal_train\n",
      "CPU times: user 375 ms, sys: 211 ms, total: 586 ms\n",
      "Wall time: 587 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# prepare dataset\n",
    "dataset_root = os.path.abspath(os.path.expanduser(IMG_DATA))\n",
    "print(f'Dataset root: {dataset_root}')\n",
    "\n",
    "img_gen_op = {'classes': CLASSES, 'target_size': IMG_SHAPE, 'batch_size': BATCH_SIZE}\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "def gen_fn(args):\n",
    "    data_path = args.decode('utf-8')\n",
    "    return image_generator.flow_from_directory(data_path,\n",
    "                                               **img_gen_op)\n",
    "\n",
    "dataset_size = dict()\n",
    "queue = [dataset_root]\n",
    "while queue:\n",
    "    path = queue.pop(0)\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_dir():\n",
    "                queue.append(entry.path)\n",
    "            if entry.is_file():\n",
    "                name = os.path.basename(os.path.dirname(os.path.dirname(entry.path)))\n",
    "                dataset_size[name] = dataset_size.get(name, 0) + 1\n",
    "\n",
    "dataset_dict = dict()\n",
    "with os.scandir(dataset_root) as it:\n",
    "    for entry in it:\n",
    "        if entry.is_dir():\n",
    "            name = os.path.basename(entry.path)\n",
    "            ds = tf.data.Dataset.from_generator(gen_fn,\n",
    "                                                args=[entry.path],\n",
    "                                                output_types=(tf.float32, tf.float32),\n",
    "                                                output_shapes=(tf.TensorShape([None, 224, 224, 3]), \n",
    "                                                               tf.TensorShape([None, len(CLASSES)]))\n",
    "                                               )\n",
    "            dataset_dict[name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180 images belonging to 4 classes.\n",
      "[[[0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "ds_series_batch = dataset_dict['0'].shuffle(20).padded_batch(6)\n",
    "\n",
    "ids, sequence_batch = next(iter(ds_series_batch))\n",
    "\n",
    "print(sequence_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(client_id):\n",
    "    return dataset_dict[client_id]\n",
    "\n",
    "client_data = tff.simulation.datasets.ClientData.from_clients_and_fn(\n",
    "                client_ids=list(dataset_dict.keys()),\n",
    "                 create_tf_dataset_for_client_fn=client_fn)\n",
    "\n",
    "train_ids = list(dataset_dict.keys())\n",
    "train_ids.remove('0')\n",
    "# train_ids = ['1'] ## for experiment client each\n",
    "dataset = [(client_data.create_tf_dataset_for_client(x), dataset_size[x]) for x in train_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<FlatMapDataset shapes: ((None, 224, 224, 3), (None, 4)), types: (tf.float32, tf.float32)>, 120)\n"
     ]
    }
   ],
   "source": [
    "example_dataset = (client_data.create_tf_dataset_for_client(client_data.client_ids[0]),\n",
    "                   dataset_size[client_data.client_ids[0]])\n",
    "print(example_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "import statistics\n",
    "# take_value = statistics.median(dataset_size.values())\n",
    "take_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 4 classes.\n",
      "(32, 224, 224, 3) (32, 4)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(dataset, take_value=None):\n",
    "#     return dataset[0].take(np.ceil(dataset[1]/BATCH_SIZE))\n",
    "    if take_value is None:\n",
    "        take_value = dataset[1]\n",
    "    else:\n",
    "        take_value = min(take_value, dataset[1])\n",
    "    return dataset[0].take(np.ceil(take_value/BATCH_SIZE))\n",
    "    \n",
    "preprocessed_example_dataset = preprocess(example_dataset, take_value)\n",
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(), iter(preprocessed_example_dataset).next())\n",
    "print(sample_batch[0].shape, sample_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_dataset = [preprocess(x, take_value) for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation\n",
    "test_dataset = (client_data.create_tf_dataset_for_client('0'), dataset_size['0'])\n",
    "federated_test_data = [preprocess(test_dataset, take_value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.FlatMapDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(client_data.create_tf_dataset_for_client('0')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: mobilenet_v3_large_075_224 : https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\n",
      "Input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mobilenet_v3_large_075_224\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
    "\n",
    "model_handle_map = {\n",
    "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
    "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
    "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
    "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
    "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
    "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
    "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
    "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
    "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
    "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
    "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
    "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
    "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
    "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
    "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
    "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
    "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
    "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
    "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
    "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
    "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
    "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
    "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
    "}\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"efficientnetv2-s\": 384,\n",
    "  \"efficientnetv2-m\": 480,\n",
    "  \"efficientnetv2-l\": 480,\n",
    "  \"efficientnetv2-b0\": 224,\n",
    "  \"efficientnetv2-b1\": 240,\n",
    "  \"efficientnetv2-b2\": 260,\n",
    "  \"efficientnetv2-b3\": 300,\n",
    "  \"efficientnetv2-s-21k\": 384,\n",
    "  \"efficientnetv2-m-21k\": 480,\n",
    "  \"efficientnetv2-l-21k\": 480,\n",
    "  \"efficientnetv2-xl-21k\": 512,\n",
    "  \"efficientnetv2-b0-21k\": 224,\n",
    "  \"efficientnetv2-b1-21k\": 240,\n",
    "  \"efficientnetv2-b2-21k\": 260,\n",
    "  \"efficientnetv2-b3-21k\": 300,\n",
    "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
    "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
    "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
    "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
    "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
    "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
    "  \"efficientnet_b0\": 224,\n",
    "  \"efficientnet_b1\": 240,\n",
    "  \"efficientnet_b2\": 260,\n",
    "  \"efficientnet_b3\": 300,\n",
    "  \"efficientnet_b4\": 380,\n",
    "  \"efficientnet_b5\": 456,\n",
    "  \"efficientnet_b6\": 528,\n",
    "  \"efficientnet_b7\": 600,\n",
    "  \"inception_v3\": 299,\n",
    "  \"inception_resnet_v2\": 299,\n",
    "  \"nasnet_large\": 331,\n",
    "  \"pnasnet_large\": 331,\n",
    "}\n",
    "\n",
    "model_handle = model_handle_map.get(model_name)\n",
    "pixels = model_image_size_map.get(model_name, 224)\n",
    "\n",
    "print(f\"Selected model: {model_name} : {model_handle}\")\n",
    "\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 32#@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_dataset(subset):\n",
    "#   return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#       '../test',\n",
    "#       validation_split=.20,\n",
    "#       subset=subset,\n",
    "#       label_mode=\"categorical\",\n",
    "#       # Seed needs to provided when using validation_split and shuffle = True.\n",
    "#       # A fixed seed is used so that the validation set is stable across runs.\n",
    "#       seed=123,\n",
    "#       image_size=IMAGE_SIZE,\n",
    "#       batch_size=1)\n",
    "\n",
    "# train_ds = build_dataset(\"training\")\n",
    "class_names = ['acne','melanoma','psoriasis','wart']\n",
    "# class_names = tuple(train_ds.class_names)\n",
    "# train_size = train_ds.cardinality().numpy()\n",
    "# train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
    "# train_ds = train_ds.repeat()\n",
    "\n",
    "# normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "# preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "# do_data_augmentation = False #@param {type:\"boolean\"}\n",
    "# if do_data_augmentation:\n",
    "#   preprocessing_model.add(\n",
    "#       tf.keras.layers.RandomRotation(40))\n",
    "#   preprocessing_model.add(\n",
    "#       tf.keras.layers.RandomTranslation(0, 0.2))\n",
    "#   preprocessing_model.add(\n",
    "#       tf.keras.layers.RandomTranslation(0.2, 0))\n",
    "#   # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n",
    "#   # image sizes are fixed when reading, and then a random zoom is applied.\n",
    "#   # If all training inputs are larger than image_size, one could also use\n",
    "#   # RandomCrop with a batch size of 1 and rebatch later.\n",
    "#   preprocessing_model.add(\n",
    "#       tf.keras.layers.RandomZoom(0.2, 0.2))\n",
    "#   preprocessing_model.add(\n",
    "#       tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
    "# train_ds = train_ds.map(lambda images, labels:\n",
    "#                         (preprocessing_model(images), labels))\n",
    "\n",
    "# val_ds = build_dataset(\"validation\")\n",
    "# valid_size = val_ds.cardinality().numpy()\n",
    "# val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
    "# val_ds = val_ds.map(lambda images, labels:\n",
    "#                     (normalization_layer(images), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fine_tuning = False #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.7.0\n",
      "Hub version: 0.9.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        # Explicitly define the input shape so the model can be properly\n",
    "        # loaded by the TFLiteConverter\n",
    "        tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "        hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(len(class_names),\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(keras_model, \n",
    "                                         input_spec=preprocessed_example_dataset.element_spec,\n",
    "#                                        dummy_batch=sample_batch,\n",
    "                                         loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                                         metrics=[tf.keras.metrics.CategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn,\n",
    "                           client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "                           server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( -> <model=<trainable=<float32[1280,4],float32[4]>,non_trainable=<float32[16],float32[72],float32[72],float32[96],float32[32],float32[1,1,144,64],float32[528],float32[32],float32[384],float32[528],float32[72],float32[192],float32[192],float32[5,5,528,1],float32[528],float32[72],float32[64],float32[384],float32[528],float32[1,1,184,720],float32[1,1,64,24],float32[72],float32[5,5,96,1],float32[1,1,32,192],float32[64],float32[144],float32[1,1,384,88],float32[1,1,96,24],float32[1,1,144,64],float32[144],float32[3,3,528,1],float32[32],float32[1,1,96,32],float32[1,1,160,64],float32[384],float32[120],float32[3,3,72,1],float32[3,3,3,16],float32[96],float32[160],float32[384],float32[1,1,88,528],float32[720],float32[3,3,64,1],float32[96],float32[64],float32[720],float32[1,1,24,72],float32[5,5,96,1],float32[64],float32[64],float32[1,1,88,528],float32[720],float32[1,1,16,16],float32[64],float32[144],float32[1,1,64,384],float32[96],float32[1,1,528,136],float32[5,5,720,1],float32[64],float32[88],float32[136],float32[1,1,16,64],float32[32],float32[32],float32[144],float32[144],float32[3,3,384,1],float32[88],float32[720],float32[120],float32[1,1,64,144],float32[384],float32[1,1,136,528],float32[720],float32[96],float32[160],float32[64],float32[64],float32[720],float32[720],float32[5,5,72,1],float32[1,1,32,96],float32[1,1,24,96],float32[3,3,160,1],float32[120],float32[24],float32[72],float32[96],float32[1,1,384,96],float32[720],float32[1,1,120,720],float32[1,1,120,720],float32[32],float32[16],float32[1,1,64,160],float32[144],float32[528],float32[120],float32[1,1,24,72],float32[1,1,96,24],float32[96],float32[192],float32[1,1,528,88],float32[88],float32[1,1,120,720],float32[1,1,720,184],float32[16],float32[96],float32[160],float32[1,1,136,528],float32[720],float32[72],float32[24],float32[96],float32[192],float32[3,3,144,1],float32[136],float32[96],float32[24],float32[88],float32[720],float32[3,3,16,1],float32[16],float32[1,1,24,72],float32[1,1,72,24],float32[1,1,720,184],float32[1,1,720,120],float32[1,1,720,1280],float32[1,1,72,32],float32[1,1,96,384],float32[120],float32[1280],float32[184],float32[64],float32[72],float32[120],float32[528],float32[1,1,72,24],float32[72],float32[1,1,96,32],float32[64],float32[160],float32[528],float32[184],float32[1,1,192,64],float32[1,1,528,136],float32[1,1,528,120],float32[720],float32[528],float32[16],float32[24],float32[24],float32[3,3,192,1],float32[144],float32[528],float32[1,1,24,96],float32[144],float32[1,1,720,120],float32[1,1,184,720],float32[96],float32[16],float32[64],float32[24],float32[3,3,144,1],float32[1,1,64,144],float32[528],float32[1,1,32,96],float32[24],float32[5,5,720,1],float32[720],float32[16],float32[96],float32[144],float32[24],float32[64],float32[88],float32[720],float32[64],float32[144],float32[144],float32[88],float32[720],float32[96],float32[72],float32[32],float32[720],float32[16],float32[64],float32[72],float32[72],float32[144],float32[528],float32[72],float32[720],float32[32],float32[528],float32[72],float32[96],float32[528],float32[120],float32[72],float32[64],float32[160],float32[720],float32[24],float32[96],float32[720],float32[16],float32[72],float32[96],float32[64],float32[88],float32[120],float32[72],float32[192],float32[160],float32[64],float32[120],float32[96],float32[120],float32[16],float32[32],float32[528],float32[720],float32[120],float32[192],float32[64],float32[32],float32[24],float32[64],float32[144],float32[384],float32[720],float32[720],float32[64],float32[144],float32[528],float32[96],float32[32],float32[528],float32[528],float32[120],float32[192],float32[144],float32[64],float32[64],float32[528],float32[720],float32[16],float32[16],float32[96],float32[384],float32[32],float32[24],float32[160],float32[384],float32[192],float32[144],float32[384],float32[88],float32[160],float32[64]>>,optimizer_state=<int64>,delta_aggregate_state=<value_sum_process=<>,weight_sum_process=<>>,model_broadcast_state=<>>@SERVER)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(iterative_process.initialize.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow_federated.python.core.impl.computation.computation_impl.ComputationImpl object at 0x7f9062656100>\n"
     ]
    }
   ],
   "source": [
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dnlab/.local/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:59: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dnlab/.local/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:59: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    }
   ],
   "source": [
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round:  1, loss: 1.4418507814407349, test_accuracy: 0.4722222089767456\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round:  2, loss: 1.0716438293457031, test_accuracy: 0.5944444537162781\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round:  3, loss: 0.8624536991119385, test_accuracy: 0.6666666865348816\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round:  4, loss: 0.7258229851722717, test_accuracy: 0.699999988079071\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round:  5, loss: 0.6260676980018616, test_accuracy: 0.7166666388511658\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round:  6, loss: 0.5710596442222595, test_accuracy: 0.7333333492279053\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round:  7, loss: 0.5135374069213867, test_accuracy: 0.7555555701255798\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round:  8, loss: 0.4772067368030548, test_accuracy: 0.7666666507720947\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round:  9, loss: 0.4498862326145172, test_accuracy: 0.7777777910232544\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 10, loss: 0.4233965575695038, test_accuracy: 0.7944444417953491\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 11, loss: 0.4041798412799835, test_accuracy: 0.7944444417953491\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 12, loss: 0.3859703540802002, test_accuracy: 0.7944444417953491\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 13, loss: 0.35825133323669434, test_accuracy: 0.7944444417953491\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 14, loss: 0.3319563865661621, test_accuracy: 0.7944444417953491\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 15, loss: 0.3186188042163849, test_accuracy: 0.8166666626930237\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 16, loss: 0.3161654770374298, test_accuracy: 0.8277778029441833\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 17, loss: 0.30389392375946045, test_accuracy: 0.8277778029441833\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 18, loss: 0.28310921788215637, test_accuracy: 0.8277778029441833\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 19, loss: 0.278247594833374, test_accuracy: 0.8333333134651184\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 20, loss: 0.2677905261516571, test_accuracy: 0.8500000238418579\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 21, loss: 0.2629598081111908, test_accuracy: 0.855555534362793\n",
      "Found 120 images belonging to 4 classes.Found 120 images belonging to 4 classes.\n",
      "\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 22, loss: 0.2613135278224945, test_accuracy: 0.8500000238418579\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 23, loss: 0.24623967707157135, test_accuracy: 0.8611111044883728\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 24, loss: 0.23800969123840332, test_accuracy: 0.8611111044883728\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 25, loss: 0.23335474729537964, test_accuracy: 0.8666666746139526\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 26, loss: 0.22952011227607727, test_accuracy: 0.8666666746139526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.Found 120 images belonging to 4 classes.\n",
      "\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 27, loss: 0.23055541515350342, test_accuracy: 0.8666666746139526\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 28, loss: 0.22346438467502594, test_accuracy: 0.8722222447395325\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 29, loss: 0.21684692800045013, test_accuracy: 0.8777777552604675\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 30, loss: 0.21019315719604492, test_accuracy: 0.8722222447395325\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 31, loss: 0.20320840179920197, test_accuracy: 0.8777777552604675\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 32, loss: 0.19596053659915924, test_accuracy: 0.8833333253860474\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 33, loss: 0.19511491060256958, test_accuracy: 0.8999999761581421\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 34, loss: 0.19159598648548126, test_accuracy: 0.8999999761581421\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 35, loss: 0.18997694551944733, test_accuracy: 0.9055555462837219\n",
      "Found 120 images belonging to 4 classes.Found 120 images belonging to 4 classes.\n",
      "\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 36, loss: 0.19055813550949097, test_accuracy: 0.9055555462837219\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 37, loss: 0.18467749655246735, test_accuracy: 0.9055555462837219\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 38, loss: 0.17558327317237854, test_accuracy: 0.9111111164093018\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 39, loss: 0.18488697707653046, test_accuracy: 0.9111111164093018\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 40, loss: 0.1718953251838684, test_accuracy: 0.9166666865348816\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 41, loss: 0.17013682425022125, test_accuracy: 0.9166666865348816\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 42, loss: 0.1671995222568512, test_accuracy: 0.9166666865348816\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 43, loss: 0.1639469563961029, test_accuracy: 0.9166666865348816\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 44, loss: 0.16430075466632843, test_accuracy: 0.9166666865348816\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.Found 120 images belonging to 4 classes.\n",
      "\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 45, loss: 0.1585557609796524, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 46, loss: 0.15786756575107574, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 47, loss: 0.1630452275276184, test_accuracy: 0.9277777671813965\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 48, loss: 0.15668083727359772, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 49, loss: 0.1517118662595749, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 50, loss: 0.1514633744955063, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 51, loss: 0.1442004144191742, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 52, loss: 0.14584967494010925, test_accuracy: 0.9222221970558167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 53, loss: 0.13913168013095856, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 54, loss: 0.14617817103862762, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 55, loss: 0.1409335732460022, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 56, loss: 0.14031630754470825, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 57, loss: 0.1378370225429535, test_accuracy: 0.9222221970558167\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 58, loss: 0.13009770214557648, test_accuracy: 0.9277777671813965\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 59, loss: 0.1376296877861023, test_accuracy: 0.9277777671813965\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 60, loss: 0.13295255601406097, test_accuracy: 0.9333333373069763\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 61, loss: 0.12964695692062378, test_accuracy: 0.9333333373069763\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 62, loss: 0.12696616351604462, test_accuracy: 0.9388889074325562\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 63, loss: 0.12861938774585724, test_accuracy: 0.9388889074325562\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 64, loss: 0.1235736683011055, test_accuracy: 0.9388889074325562\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 65, loss: 0.1250104308128357, test_accuracy: 0.9388889074325562\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.Found 120 images belonging to 4 classes.\n",
      "\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 66, loss: 0.130739226937294, test_accuracy: 0.9333333373069763\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 67, loss: 0.12434802949428558, test_accuracy: 0.9333333373069763\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 68, loss: 0.12022913247346878, test_accuracy: 0.9388889074325562\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 69, loss: 0.1183912381529808, test_accuracy: 0.9388889074325562\n",
      "Found 120 images belonging to 4 classes.Found 120 images belonging to 4 classes.\n",
      "\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 70, loss: 0.12263361364603043, test_accuracy: 0.9333333373069763\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 71, loss: 0.12019770592451096, test_accuracy: 0.9388889074325562\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 72, loss: 0.12272338569164276, test_accuracy: 0.9333333373069763\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 73, loss: 0.11542364954948425, test_accuracy: 0.9388889074325562\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 120 images belonging to 4 classes.\n",
      "Found 180 images belonging to 4 classes.\n",
      "round: 74, loss: 0.11731401830911636, test_accuracy: 0.9388889074325562\n"
     ]
    }
   ],
   "source": [
    "NUM_ROUNDS = 1000\n",
    "MAX_STD = 0.001\n",
    "loss = list()\n",
    "accuracy = list()\n",
    "\n",
    "val_loss = list()\n",
    "val_accuracy = list()\n",
    "\n",
    "max = 0\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "for round_num in range(1, NUM_ROUNDS+1):\n",
    "    state, metrics = iterative_process.next(state, federated_dataset)\n",
    "    val_metrics = evaluation(state.model, federated_test_data)\n",
    "    loss.append(metrics['train']['loss'])\n",
    "    accuracy.append(metrics['train']['categorical_accuracy'])\n",
    "    val_loss.append(val_metrics['loss'])\n",
    "    val_accuracy.append(val_metrics['categorical_accuracy'])\n",
    "    print(f'round: {round_num:2d}, loss: {metrics[\"train\"][\"loss\"]}, test_accuracy: {val_metrics[\"categorical_accuracy\"]}')\n",
    "    #가장 높은 확률일 때 모델 세이브\n",
    "    if(val_metrics[\"categorical_accuracy\"] > max):\n",
    "        create_keras_model().save_weights(\"ckpt\")\n",
    "        \n",
    "#     create_keras_model().save(create_keras_model().set_weights(create_keras_model().get_weights()))\n",
    "    #         create_keras_model().save('./save_model') #keras pb\n",
    "    \n",
    "    if len(val_loss) > 3 and np.std(val_loss[-3:]) < MAX_STD:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_keras_model().set_weights(create_keras_model().get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################   pred ##################################################\n",
    "\n",
    "#마지막 레이어 확률 값 \n",
    "# probability_model = tf.keras.Sequential([create_keras_model(), \n",
    "#                                          tf.keras.layers.Softmax()])\n",
    "from keras.models import load_model\n",
    "# model = create_keras_model()\n",
    "# model.set_weights(state.model.trainable)\n",
    "# model.predict()\n",
    "probability_model = tf.keras.models.load_model('./save')\n",
    "print(probability_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probability_model.predict(federated_test_data[0]))\n",
    "predictions = probability_model.predict(federated_test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 데이터 라벨 맞춘 개수\n",
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "d = 0\n",
    "for i in range(180):\n",
    "    print(predictions[i])\n",
    "    n = np.argmax(predictions[i])\n",
    "    # print(np.argmax(predictions[i]))\n",
    "    if n==0:\n",
    "      a = a+1\n",
    "    elif n==1 :\n",
    "      b = b+1\n",
    "    elif n==2 :\n",
    "      c = c+1\n",
    "    elif n==3:\n",
    "      d = d+1\n",
    "print(a,\" \",b,\" \",c,\" \",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #실제 라벨 리스트 만들기\n",
    "# actual_value = tf.keras.Sequential([create_keras_model(),tf.keras.layers.InputLayer()])\n",
    "# actuals = actual_value.predict(federated_test_data[0])\n",
    "# print(actuals)\n",
    "# a = 0\n",
    "# b = 0\n",
    "# c = 0\n",
    "# d = 0\n",
    "# for i in range(180):\n",
    "#     print(actuals[i])\n",
    "#     n = np.argmax(actuals[i])\n",
    "#     # print(np.argmax(predictions[i]))\n",
    "#     if n==0:\n",
    "#       a = a+1\n",
    "#     elif n==1 :\n",
    "#       b = b+1\n",
    "#     elif n==2 :\n",
    "#       c = c+1\n",
    "#     elif n==3:\n",
    "#       d = d+1\n",
    "# print(a,\" \",b,\" \",c,\" \",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################################   pred ##################################################\n",
    "\n",
    "#마지막 레이어 확률 값 \n",
    "# probability_model = tf.keras.Sequential([create_keras_model(), \n",
    "#                                          tf.keras.layers.Softmax()])\n",
    "from keras.models import load_model\n",
    "# model = create_keras_model()\n",
    "# model.set_weights(state.model.trainable)\n",
    "# model.predict()\n",
    "probability_model = tf.keras.models.load_model('./save_model')\n",
    "print(probability_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probability_model.predict(federated_test_data[0]))\n",
    "predictions = probability_model.predict(federated_test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 데이터 라벨 맞춘 개수\n",
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "d = 0\n",
    "for i in range(180):\n",
    "    print(predictions[i])\n",
    "    n = np.argmax(predictions[i])\n",
    "    # print(np.argmax(predictions[i]))\n",
    "    if n==0:\n",
    "      a = a+1\n",
    "    elif n==1 :\n",
    "      b = b+1\n",
    "    elif n==2 :\n",
    "      c = c+1\n",
    "    elif n==3:\n",
    "      d = d+1\n",
    "print(a,\" \",b,\" \",c,\" \",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### 데이터셋 라벨 확인할 것 ##############################\n",
    "# print(test_dataset.index('acne'))\n",
    "ac = probability_model.predict(federated_dataset[1])\n",
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "d = 0\n",
    "for i in range(143):\n",
    "    print(ac[i])\n",
    "    n = np.argmax(ac[i])\n",
    "    print(np.argmax(ac[i]))\n",
    "    if n==0:\n",
    "      a = a+1\n",
    "    elif n==1 :\n",
    "      b = b+1\n",
    "    elif n==2 :\n",
    "      c = c+1\n",
    "    elif n==3:\n",
    "      d = d+1\n",
    "print(a,\" \",b,\" \",c,\" \",d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig1.add_subplot(2, 1, 1)\n",
    "ax1.plot(accuracy, label='Training Accuracy')\n",
    "ax1.plot(val_accuracy, label='Validation Accuracy')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Training and Validation Accuracy')\n",
    "\n",
    "# ax2 = fig1.add_subplot(2, 1, 2)\n",
    "# ax2.plot(loss, label='Training Loss')\n",
    "# ax2.plot(val_loss, label='Validation Loss')\n",
    "# ax2.legend(loc='upper right')\n",
    "# ax2.set_ylabel('Cross Entropy')\n",
    "# ax2.set_ylim([0,max(ax2.get_ylim())])\n",
    "# ax2.set_title('Training and Validation Loss')\n",
    "# ax2.set_xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('output.pickle', 'wb') as f:\n",
    "    pickle.dump((accuracy, val_accuracy, loss, val_loss), f)\n",
    "with open('output.pickle', 'rb') as f:\n",
    "    print(max(pickle.load(f)[1]))\n",
    "print(max(val_accuracy))\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# print(\"pandas version: \", pd.__version__)\n",
    "# pd.set_option('display.max_row', 5000)\n",
    "# pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(iterative_process.initialize())\n",
    "\n",
    "print(iterative_process.initialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(iterative_process.next(state, federated_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_metrics['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 배열이 일부만 표시됨.\n",
    "\n",
    " \n",
    "\n",
    "# 해결방법\n",
    "\n",
    "# import sys\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    " \n",
    "\n",
    "# 옵션을 바꿔주면 배열 전체가 표시됨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "# import seaborn as sns\n",
    " \n",
    "# cm2 = confusion_matrix(comp_df['실제값'],comp_df['예측값'])\n",
    "# cmdf2 = DataFrame(cm2, index=['실제값(N)', '실제값(P)'], columns=['예측값(N)', '예측값(P)'])\n",
    "# cmdf2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(federated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "model = create_keras_model()\n",
    "model.save('./my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pb -> tflite \n",
    "from tensorflow import keras\n",
    "#허브모델 사용시\n",
    "model = tf.keras.models.load_model('./save_model/', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "#기본 모델 사용시\n",
    "# model = tf.keras.models.load_model('./my_model.h5')\n",
    "export_path = './pb'\n",
    "model.save(export_path, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################3 누군가 사용한 UINT8만드는 converter\n",
    "def representative_dataset_gen():\n",
    "    for _ in range(10):\n",
    "        dummy_image = tf.random.uniform([1, 224, 224, 3], 0., 255., dtype=tf.float32)\n",
    "        dummy_image = dummy_image / 127.5 - 1\n",
    "        yield [dummy_image]\n",
    "saved_model_dir = './save/'\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "open('./mobileNetv2_uint8.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "# print('TensorFlow Model is  {} bytes'.format(tf_model_size))\n",
    "# print('TFLite Model is      {} bytes'.format(tflite_model_size))\n",
    "# print('Post training int8 quantization saves {} bytes'.format(tf_model_size-tflite_model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saved_model_dir = './save/'\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "open('./mobileNetv2.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 17:00:45.797985: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_1378926) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_1378910) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2021-12-07 17:01:00.826365: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2021-12-07 17:01:00.826391: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2021-12-07 17:01:00.826396: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.\n",
      "2021-12-07 17:01:03.845508: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1891] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexBroadcastGradientArgs\n",
      "Details:\n",
      "\ttf.BroadcastGradientArgs(tensor<2xi32>, tensor<2xi32>) -> (tensor<?xi32>, tensor<?xi32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "\n",
    "IMG_SIZE = 224\n",
    "NUM_FEATURES = 7 * 7 * 1280\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "class TransferLearningModel(tf.Module):\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        self.num_features = NUM_FEATURES\n",
    "        self.num_classes = NUM_CLASSES\n",
    "\n",
    "        # trainable weights and bias for softmax\n",
    "        self.ws = tf.Variable(\n",
    "            tf.zeros((self.num_features, self.num_classes)),\n",
    "            name='ws',\n",
    "            trainable=True)\n",
    "        self.bs = tf.Variable(\n",
    "            tf.zeros((1, self.num_classes)), name='bs', trainable=True)\n",
    "\n",
    "        # base model\n",
    "        self.base = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "            alpha=1.0,\n",
    "            include_top=False,\n",
    "            weights= \"imagenet\")\n",
    "        # loss function and optimizer\n",
    "        self.loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "          tf.TensorSpec([None, IMG_SIZE, IMG_SIZE, 3], tf.float32),\n",
    "      ])\n",
    "    def load(self, feature):\n",
    "        x = tf.keras.applications.mobilenet_v2.preprocess_input(\n",
    "            tf.multiply(feature, 255))\n",
    "        bottleneck = tf.reshape(\n",
    "            self.base(x, training=False), (-1, self.num_features))\n",
    "        return {'bottleneck': bottleneck}\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "          tf.TensorSpec([None, NUM_FEATURES], tf.float32),\n",
    "          tf.TensorSpec([None, NUM_CLASSES], tf.float32),\n",
    "      ])\n",
    "    def train(self, bottleneck, label):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = tf.matmul(bottleneck, self.ws) + self.bs\n",
    "            prediction = tf.nn.softmax(logits)\n",
    "            loss = self.loss_fn(prediction, label)\n",
    "        gradients = tape.gradient(loss, [self.ws, self.bs])\n",
    "        self.optimizer.apply_gradients(zip(gradients, [self.ws, self.bs]))\n",
    "        result = {'loss': loss}\n",
    "        for grad in gradients:\n",
    "            result[grad.name] = grad\n",
    "        return result\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "          tf.TensorSpec([None, IMG_SIZE, IMG_SIZE, 3], tf.float32)\n",
    "          ])\n",
    "    def infer(self, feature):\n",
    "        x = tf.keras.applications.mobilenet_v2.preprocess_input(\n",
    "            tf.multiply(feature, 255))\n",
    "        bottleneck = tf.reshape(\n",
    "            self.base(x, training=False), (-1, self.num_features))\n",
    "        logits = tf.matmul(bottleneck, self.ws) + self.bs\n",
    "        return {'output': tf.nn.softmax(logits)}\n",
    "\n",
    "def convert_and_save(saved_model_dir='saved_model'):\n",
    "    model = TransferLearningModel()\n",
    "#학습이 안되어있음.얹을 방법이 없나???\n",
    "    tf.saved_model.save(\n",
    "          model,\n",
    "          saved_model_dir,\n",
    "          signatures={\n",
    "              'load': model.load.get_concrete_function(),\n",
    "              'train': model.train.get_concrete_function(),\n",
    "              'infer': model.infer.get_concrete_function(),   \n",
    "          })\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "    converter.target_spec.supported_ops = [\n",
    "          tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "          tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "      ]\n",
    "    converter.experimental_enable_resource_variables = True\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    model_file_path = os.path.join('model.tflite')\n",
    "    with open(model_file_path, 'wb') as model_file:\n",
    "        model_file.write(tflite_model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    convert_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
